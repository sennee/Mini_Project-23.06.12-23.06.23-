{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 세트 정확도: 0.9907739453608919\n",
      "검증 세트 정확도: 0.9861340519864383\n",
      "test 데이터 예측 [1 1 1 ... 0 0 0]\n",
      "       year  month  day  hour  min  target\n",
      "0      2022      8   24    14   41       1\n",
      "1      2022      8   24    14   42       1\n",
      "2      2022      8   24    14   43       1\n",
      "3      2022      8   24    14   44       1\n",
      "4      2022      8   24    14   45       1\n",
      "...     ...    ...  ...   ...  ...     ...\n",
      "51132  2023      1   17    19   44       0\n",
      "51133  2023      1   17    19   50       0\n",
      "51134  2023      1   17    19   56       0\n",
      "51135  2023      1   17    19   59       0\n",
      "51136  2023      1   17    20    5       0\n",
      "\n",
      "[51137 rows x 6 columns]\n",
      "(array([0, 1], dtype=int64), array([20694, 30443], dtype=int64))\n",
      "훈련 세트에 대한 csi 98.52823908747357\n",
      "검증 세트에 대한 csi 97.79954473339312\n"
     ]
    }
   ],
   "source": [
    "#공통 import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "\n",
    "from joblib import parallel_backend\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#모델 import\n",
    "from xgboost import XGBRFClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "# df_train = pd.read_csv('./통합데이터/df_train_v3_mean.csv')\n",
    "# df_train = pd.read_csv('./통합데이터/df_train_v3.csv')\n",
    "# df_train = pd.read_csv('./통합데이터/df_train_v3_mean_1m.csv')\n",
    "df_train = pd.read_csv('./df_train_v3_mean_1m_이상치제거.csv')\n",
    "\n",
    "df_test = pd.read_csv(\"./통합데이터/df_test_v4_mean.csv\")\n",
    "\n",
    "'''\n",
    "# 시간 컬럼 제거\n",
    "df_test_drop = df_test.drop(columns=['year', 'month', 'day', 'hour', 'min'])\n",
    "\n",
    "# 데이터 전처리\n",
    "data = df_train.drop(columns=['answer', 'year', 'month', 'day', 'hour'])\n",
    "target = df_train['answer']\n",
    "'''\n",
    "\n",
    "# 입력 데이터, 타깃 데이터, 테스트 데이터 설정\n",
    "data = df_train[['lat', 'lon', 'sog', 'cog', 'hdg', 'khnp_buoy.ws', 'khnp_buoy.wd', 'kma_lightbecon.ws', 'kma_lightbecon.wd', 'kma_pagobuoy.mean_wh']].to_numpy()\n",
    "target = df_train['answer'].to_numpy()\n",
    "\n",
    "test_data = df_test[['lat', 'lon', 'sog', 'cog', 'hdg','khnp_buoy.ws', 'khnp_buoy.wd', 'kma_lightbecon.ws', 'kma_lightbecon.wd', 'kma_pagobuoy.mean_wh']].to_numpy()\n",
    "\n",
    "# 스케일링\n",
    "scaler = StandardScaler() #<- 스케일링 방식 변경 가능\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "# df_test_scaled = scaler.transform(df_test_drop)\n",
    "df_test_scaled = scaler.transform(test_data)\n",
    "\n",
    "'''\n",
    "# PCA를 사용하여 차원 축소\n",
    "pca = PCA(n_components = 10)  # 목표 차원 수로 조정\n",
    "data_pca = pca.fit_transform(data_scaled)\n",
    "\n",
    "# 축소된 데이터로 새로운 데이터프레임 생성\n",
    "df_pca = pd.DataFrame(data_pca, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10'])\n",
    "'''\n",
    "\n",
    "# 축소된 데이터로 모델 훈련 데이터 나누기\n",
    "# X_train, X_val, y_train, y_val = train_test_split(df_pca, target, test_size=0.2, random_state=123, stratify=target)\n",
    "X_train, X_val, y_train, y_val = train_test_split(data_scaled, target, test_size=0.2, random_state=123, stratify=target)\n",
    "\n",
    "# 모델 객체 생성\n",
    "rfc = RandomForestClassifier(n_estimators=100, max_depth=13, n_jobs=-1, random_state=42) # max_depth 13으로 바꿈\n",
    "adaboost = AdaBoostClassifier(random_state=42)\n",
    "xgbrfc = XGBRFClassifier()\n",
    "\n",
    "# 모델 훈련\n",
    "rfc.fit(X_train, y_train)\n",
    "# adaboost(X_train, y_train)\n",
    "# xgbrfc(X_train, y_train)\n",
    "\n",
    "# 훈련 세트 정확도 출력\n",
    "train_pred = rfc.predict(X_train)\n",
    "train_accuracy = accuracy_score(y_train, train_pred)\n",
    "print(\"훈련 세트 정확도:\", train_accuracy)\n",
    "\n",
    "# 검증 세트 정확도 출력\n",
    "val_pred = rfc.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, val_pred)\n",
    "print(\"검증 세트 정확도:\", val_accuracy)\n",
    "\n",
    "# test 데이터 예측\n",
    "print(\"test 데이터 예측\", rfc.predict(df_test_scaled))\n",
    "\n",
    "# 예측 결과 dataframe 생성\n",
    "pd.DataFrame({'year':df_test.year, 'month':df_test.month, 'day':df_test.day, 'hour':df_test.hour, 'min':df_test['min'], 'target':rfc.predict(df_test_scaled)}).to_csv('result(scaled).csv', index=False)\n",
    "\n",
    "# 예측 결과 확인\n",
    "result = pd.read_csv('./제출 양식/result(scaled).csv')\n",
    "print(result)\n",
    "\n",
    "# 예측 결과 편중 확인\n",
    "x = rfc.predict(df_test_scaled)\n",
    "print(np.unique(x, return_counts=True))\n",
    "\n",
    "# CSI 계산\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def calculate_csi(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    csi = tp / (tp + fn + fp) * 100\n",
    "    return csi\n",
    "\n",
    "print(\"훈련 세트에 대한 csi\", calculate_csi(y_train, train_pred))\n",
    "print(\"검증 세트에 대한 csi\", calculate_csi(y_val, val_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
